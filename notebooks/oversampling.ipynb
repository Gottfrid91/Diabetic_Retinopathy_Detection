{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook implements oversampling to be included in the DRD Deep learning script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Routine for decoding the CIFAR-10 binary file format.\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import argparse\n",
    "import tensorflow as tf\n",
    "\n",
    "# Process images of this size. Note that this differs from the original CIFAR\n",
    "# image size of 32 x 32. If one alters this number, then the entire model\n",
    "# architecture will change and any model would need to be retrained.\n",
    "IMAGE_SIZE = 224\n",
    "\n",
    "# Global constants describing the CIFAR-10 data set.\n",
    "NUM_CLASSES = 5\n",
    "NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN = 2000 # was set from # 50000\n",
    "NUM_EXAMPLES_PER_EPOCH_FOR_EVAL = 3500\n",
    "batch_size = 100\n",
    "data_dir = '/home/olle/PycharmProjects/Diabetic_Retinopathy_Detection/data/train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read in an image to the class object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "record bytes:::::  Tensor(\"Reshape:0\", shape=(256, 256, 3), dtype=uint8)\n"
     ]
    }
   ],
   "source": [
    "filenames = [os.path.join(data_dir, 'data_batch_%d.bin' % i)\n",
    "             for i in xrange(0, 7)]\n",
    "for f in filenames:\n",
    "    if not tf.gfile.Exists(f):\n",
    "        raise ValueError('Failed to find file: ' + f)\n",
    "num_examples_per_epoch = NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN\n",
    "\n",
    "# Create a queue that produces the filenames to read.\n",
    "filename_queue = tf.train.string_input_producer(filenames)\n",
    "\n",
    "class SVHNRecord(object):\n",
    "    pass\n",
    "\n",
    "result = SVHNRecord()\n",
    "\n",
    "# Dimensions of the images in the SVHN dataset.\n",
    "# See http://ufldl.stanford.edu/housenumbers/ for a description of the\n",
    "# input format.\n",
    "result.height = 256\n",
    "result.width = 256\n",
    "result.depth = 3\n",
    "\n",
    "reader = tf.TFRecordReader()\n",
    "result.key, value = reader.read(filename_queue)\n",
    "value = tf.parse_single_example(\n",
    "    value,\n",
    "    # Defaults are not specified since both keys are required.\n",
    "    features={\n",
    "        'image_raw': tf.FixedLenFeature(shape=[], dtype=tf.string),\n",
    "        'label': tf.FixedLenFeature(shape=[], dtype=tf.int64),\n",
    "        'image_name': tf.FixedLenFeature(shape=[], dtype=tf.string)\n",
    "    })\n",
    "\n",
    "# Convert from a string to a vector of uint8 that is record_bytes long.\n",
    "record_bytes = tf.decode_raw(value['image_raw'], tf.uint8)\n",
    "# record_bytes.set_shape([32*32*3])\n",
    "record_bytes = tf.reshape(record_bytes, [256, 256, 3])\n",
    "print(\"record bytes::::: \", record_bytes)\n",
    "# Store our label to result.label and convert to int32\n",
    "result.label = tf.cast(value['label'], tf.int32)\n",
    "result.uint8image = record_bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read examples from files in the filename queue.\n",
    "reshaped_image = tf.cast(result.uint8image, tf.float32)\n",
    "\n",
    "height = IMAGE_SIZE\n",
    "width = IMAGE_SIZE\n",
    "\n",
    "# Image processing for evaluation.\n",
    "# Crop the central [height, width] of the image.\n",
    "resized_image = tf.image.resize_image_with_crop_or_pad(reshaped_image,\n",
    "                                                       height, width)\n",
    "\n",
    "# Subtract off the mean and divide by the variance of the pixels.\n",
    "float_image = tf.divide(resized_image, 255)\n",
    "\n",
    "# Set the shapes of tensors.\n",
    "float_image.set_shape([height, width, 3])\n",
    "# read_input.label.set_shape([1])\n",
    "\n",
    "# Ensure that the random shuffling has good mixing properties.\n",
    "min_fraction_of_examples_in_queue = 0.4\n",
    "min_queue_examples = int(num_examples_per_epoch *\n",
    "                         min_fraction_of_examples_in_queue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _generate_image_and_label_batch(images_and_labels, min_queue_examples,\n",
    "                                    batch_size, shuffle):\n",
    "\n",
    "    # Create a queue that shuffles the examples, and then\n",
    "    # read 'batch_size' images + labels from the example queue.\n",
    "    num_preprocess_threads = 16\n",
    "    images, label_batch = tf.train.batch(\n",
    "        [image, label],\n",
    "        batch_size=batch_size,\n",
    "        num_threads=num_preprocess_threads,\n",
    "        capacity=min_queue_examples + 3 * batch_size)\n",
    "\n",
    "    # Display the training images in the visualizer.\n",
    "    tf.summary.image('images', images)\n",
    "\n",
    "    return images, tf.reshape(label_batch, [batch_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = _generate_image_and_label_batch(images_and_labels,\n",
    "                                           min_queue_examples, batch_size,\n",
    "                                           shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "images_and_labels=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot infer Tensor's rank: Tensor(\"cond_22/Merge:0\", dtype=int32)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-114-aa4d92612dfc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mcapacity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_preprocess_threads\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mmin_after_dequeue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_preprocess_threads\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     enqueue_many=True)\n\u001b[0m",
      "\u001b[0;32m/home/olle/anaconda3/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/training/input.pyc\u001b[0m in \u001b[0;36mshuffle_batch_join\u001b[0;34m(tensors_list, batch_size, capacity, min_after_dequeue, seed, enqueue_many, shapes, allow_smaller_final_batch, shared_name, name)\u001b[0m\n\u001b[1;32m   1453\u001b[0m       \u001b[0mallow_smaller_final_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_smaller_final_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1454\u001b[0m       \u001b[0mshared_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshared_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1455\u001b[0;31m       name=name)\n\u001b[0m\u001b[1;32m   1456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/olle/anaconda3/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/training/input.pyc\u001b[0m in \u001b[0;36m_shuffle_batch_join\u001b[0;34m(tensors_list, batch_size, capacity, min_after_dequeue, keep_input, seed, enqueue_many, shapes, allow_smaller_final_batch, shared_name, name)\u001b[0m\n\u001b[1;32m    869\u001b[0m         tensor_list_list, enqueue_many, keep_input)\n\u001b[1;32m    870\u001b[0m     \u001b[0mtypes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_dtypes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_list_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 871\u001b[0;31m     \u001b[0mshapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_shapes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_list_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshapes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menqueue_many\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    872\u001b[0m     queue = data_flow_ops.RandomShuffleQueue(\n\u001b[1;32m    873\u001b[0m         \u001b[0mcapacity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcapacity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_after_dequeue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmin_after_dequeue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/olle/anaconda3/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/training/input.pyc\u001b[0m in \u001b[0;36m_shapes\u001b[0;34m(tensor_list_list, shapes, enqueue_many)\u001b[0m\n\u001b[1;32m    677\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtl\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndims\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 679\u001b[0;31m           \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot infer Tensor's rank: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtl\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m     shapes = [_merge_shapes(\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot infer Tensor's rank: Tensor(\"cond_22/Merge:0\", dtype=int32)"
     ]
    }
   ],
   "source": [
    "# Convert 3D tensor of shape [height, width, channels] to \n",
    "# a 4D tensor of shape [batch_size, height, width, channels]\n",
    "image = tf.expand_dims(float_image, 0)\n",
    "label = tf.expand_dims(result.label, 0)\n",
    "# Define the boolean predicate to be true when the class label is 1\n",
    "pred = tf.equal(result.label, tf.convert_to_tensor([1]))\n",
    "pred = tf.reshape(pred, [])\n",
    "\n",
    "oversample_factor = 2\n",
    "r_image = tf.cond(pred, lambda: tf.concat([image]*oversample_factor,0), lambda: image)\n",
    "r_label = tf.cond(pred, lambda: tf.stack([label]*oversample_factor, 0), lambda: label)\n",
    "\n",
    "images_and_labels.append([r_image, r_label])\n",
    "\n",
    "num_preprocess_threads = 16\n",
    "\n",
    "images, label_batch = tf.train.shuffle_batch_join(\n",
    "    images_and_labels,\n",
    "    batch_size=batch_size,\n",
    "    capacity=2 * num_preprocess_threads * batch_size,\n",
    "    min_after_dequeue=1 * num_preprocess_threads * batch_size,\n",
    "    enqueue_many=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'ExpandDims_31:0' shape=(1,) dtype=int32>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Built graph to execute data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # Build an initialization operation to run below.\n",
    "init = tf.global_variables_initializer()\n",
    "# Start running operations on the Graph.\n",
    "sess = tf.Session()\n",
    "# sess.run(init)\n",
    "\n",
    "# Start the queue runners.\n",
    "tf.train.start_queue_runners(sess=sess)\n",
    "\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Operation u'batch_2' has been marked as not fetchable.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-109-b57ecbd66e9b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/olle/anaconda3/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/olle/anaconda3/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1120\u001b[0m     \u001b[0;31m# Create a fetch handler to take care of the structure of fetches.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1121\u001b[0m     fetch_handler = _FetchHandler(\n\u001b[0;32m-> 1122\u001b[0;31m         self._graph, fetches, feed_dict_tensor, feed_handles=feed_handles)\n\u001b[0m\u001b[1;32m   1123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m     \u001b[0;31m# Run request and get response.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/olle/anaconda3/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, graph, fetches, feeds, feed_handles)\u001b[0m\n\u001b[1;32m    438\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_assert_fetchable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/olle/anaconda3/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_assert_fetchable\u001b[0;34m(self, graph, op)\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_fetchable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m       raise ValueError(\n\u001b[0;32m--> 453\u001b[0;31m           'Operation %r has been marked as not fetchable.' % op.name)\n\u001b[0m\u001b[1;32m    454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Operation u'batch_2' has been marked as not fetchable."
     ]
    }
   ],
   "source": [
    "sess.run(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights =  1/prop\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def undersampling():\n",
    "    \n",
    "    np.argwhere(prop==np.max(prop, axis=0))\n",
    "    value_to_be_removed = values[prop==np.max(prop, axis=0)]\n",
    "    index_to_delete = np.where(cls_true==value_to_be_removed)[0][0]\n",
    "    new_cls = np.delete(cls_true, index_to_delete)\n",
    "    \n",
    "    return(new_cls)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_np.shape\n",
    "value['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "class_num=8\n",
    "data_np = np.random.choice(class_num,20000,p=[1.0/class_num]*class_num)\n",
    "\n",
    "def sample(_):\n",
    "    xx = tf.cast(tf.random_uniform([1])*class_num, tf.int32)[0]\n",
    "    return xx\n",
    "\n",
    "def fix_sample_and_rebalance(which='works'): \n",
    "    data_tensors = tf.constant(data_np, dtype=tf.int32)\n",
    "    #data_tensors = images\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(data_tensors)\n",
    "\n",
    "    target_dist = [1.0/class_num] * class_num\n",
    "    target_dist[1]+=target_dist[0] ; target_dist[0]=0\n",
    "    print('target-dist>>', target_dist)\n",
    "\n",
    "    if which == 'breaks':\n",
    "        dataset = dataset.map(sample)\n",
    "\n",
    "    dataset = dataset.apply(tf.contrib.data.rejection_resample(\n",
    "      class_func=lambda c: c,\n",
    "      target_dist=target_dist))\n",
    "    dataset = dataset.map(lambda a, _: a)\n",
    "\n",
    "    return dataset.make_one_shot_iterator().get_next()\n",
    "\n",
    "def run_thing(which):\n",
    "    tf.reset_default_graph()\n",
    "    with tf.Session() as sess:\n",
    "        get_next = fix_sample_and_rebalance(which)\n",
    "        returned = []\n",
    "        for kk in range(0,100):\n",
    "            try:\n",
    "                sample=sess.run(get_next)\n",
    "                returned.append(sample)\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                break\n",
    "    print(np.bincount(np.array(returned)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target-dist>> [0, 0.25, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125]\n",
      "[ 0 29  8 10 12 19 13  9]\n",
      "target-dist>> [0, 0.25, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125]\n",
      "[16 11 14 16 10 14 11  8]\n"
     ]
    }
   ],
   "source": [
    "run_thing('works')  # this works\n",
    "run_thing('breaks')  # this doesn't"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Incompatible return types of true_fn and false_fn: The two structures don't have the same sequence type. First structure has type <type 'tuple'>, while second structure has type <type 'list'>.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-119-780d6c519bfb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# Here we have N conditionals, one for each class.  These are exclusive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# but due to tf.case() not behaving every conditional gets evaluated.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0;34m[\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcond\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcond\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcond\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/olle/anaconda3/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/util/deprecation.pyc\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    430\u001b[0m                 \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m                 instructions)\n\u001b[0;32m--> 432\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m     return tf_decorator.make_decorator(func, new_func, 'deprecated',\n\u001b[1;32m    434\u001b[0m                                        _add_deprecated_arg_notice_to_docstring(\n",
      "\u001b[0;32m/home/olle/anaconda3/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.pyc\u001b[0m in \u001b[0;36mcond\u001b[0;34m(pred, true_fn, false_fn, strict, name, fn1, fn2)\u001b[0m\n\u001b[1;32m   2040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2041\u001b[0m       raise TypeError(\n\u001b[0;32m-> 2042\u001b[0;31m           \"Incompatible return types of true_fn and false_fn: {}\".format(e))\n\u001b[0m\u001b[1;32m   2043\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2044\u001b[0m       raise ValueError(\n",
      "\u001b[0;31mTypeError\u001b[0m: Incompatible return types of true_fn and false_fn: The two structures don't have the same sequence type. First structure has type <type 'tuple'>, while second structure has type <type 'list'>."
     ]
    }
   ],
   "source": [
    "# Assume your oversampling factors per class are fixed\n",
    "# and you have 4 classes.\n",
    "OVERSAMPLE_FACTOR = [1,1,1,1,1]\n",
    "label= tf.cast(result.label, tf.int32)\n",
    "images = tf.cast(result.uint8image, tf.int32)\n",
    "# Now we need to reshape input image tensors to 4D, where the \n",
    "# first dimension is the image number in a batch of oversampled tensors.\n",
    "label = tf.reshape(label,[1]) # so, (*,height,width,channels) in 4D\n",
    "\n",
    "# Set up your predicates, which are 1D boolean tensors.\n",
    "# Note you will have to squash the boolean tensors to 0-dimension.\n",
    "# This seems illogical to me, but it is what it is.\n",
    "pred0 = tf.reshape(tf.equal(label, tf.convert_to_tensor([0])), []) #0D tf.bool\n",
    "pred1 = tf.reshape(tf.equal(label, tf.convert_to_tensor([1])), []) #0D tf.bool\n",
    "pred2 = tf.reshape(tf.equal(label, tf.convert_to_tensor([2])), []) #0D tf.bool\n",
    "pred3 = tf.reshape(tf.equal(label, tf.convert_to_tensor([3])), []) #0D tf.bool\n",
    "pred4 = tf.reshape(tf.equal(label, tf.convert_to_tensor([4])), []) #0D tf.bool\n",
    "\n",
    "# Build your callables (functions) that vertically stack an input image and\n",
    "# label tensors X times depending on the accompanying oversample factor.\n",
    "def f0(): return tf.concat( [images]*OVERSAMPLE_FACTOR[0],0), tf.concat([label]*OVERSAMPLE_FACTOR[0],0)\n",
    "def f1(): return tf.concat( [images]*OVERSAMPLE_FACTOR[1],0), tf.concat([label]*OVERSAMPLE_FACTOR[1],0)\n",
    "def f2(): return tf.concat( [images]*OVERSAMPLE_FACTOR[2],0), tf.concat([label]*OVERSAMPLE_FACTOR[2],0)\n",
    "def f3(): return tf.concat( [images]*OVERSAMPLE_FACTOR[3],0), tf.concat([label]*OVERSAMPLE_FACTOR[3],0)\n",
    "def f4(): return tf.concat( [images]*OVERSAMPLE_FACTOR[4],0), tf.concat([label]*OVERSAMPLE_FACTOR[4],0)\n",
    "\n",
    "# Here we have N conditionals, one for each class.  These are exclusive\n",
    "# but due to tf.case() not behaving every conditional gets evaluated.\n",
    "[images, label] = tf.cond(pred0, f0, lambda: [images,label])\n",
    "[images, label] = tf.cond(pred1, f1, lambda: [images,label])\n",
    "[images, label] = tf.cond(pred2, f2, lambda: [images,label])\n",
    "[images, label] = tf.cond(pred3, f3, lambda: [images,label])\n",
    "[images, label] = tf.cond(pred3, f4, lambda: [images,label])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
