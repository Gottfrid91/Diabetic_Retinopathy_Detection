{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building an input pipeline with the Dataset API\n",
    "\n",
    "This scripts builds an input pipeline using the Dataset API from tensorflow. It preforms the following tasks:\n",
    "1. parses images from TFRecords\n",
    "2. rezises and normalises the input\n",
    "3. shuffles the dataset and returns a batch of size \"batch_size\"\n",
    "4. trough an iterator provides access inside a session\n",
    "\n",
    "Further this notebook also implements the functions parse and input_fn. These functions take an dataset batch and select one element from each class, thereby performing uniform smapling of each class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "from matplotlib.image import imread\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.framework import dtypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/olle/PycharmProjects/Diabetic_Retinopathy_Detection/data/train/data_batch_0.bin', '/home/olle/PycharmProjects/Diabetic_Retinopathy_Detection/data/train/data_batch_1.bin', '/home/olle/PycharmProjects/Diabetic_Retinopathy_Detection/data/train/data_batch_2.bin', '/home/olle/PycharmProjects/Diabetic_Retinopathy_Detection/data/train/data_batch_3.bin', '/home/olle/PycharmProjects/Diabetic_Retinopathy_Detection/data/train/data_batch_4.bin', '/home/olle/PycharmProjects/Diabetic_Retinopathy_Detection/data/train/data_batch_5.bin', '/home/olle/PycharmProjects/Diabetic_Retinopathy_Detection/data/train/data_batch_6.bin']\n"
     ]
    }
   ],
   "source": [
    "data_dir = '/home/olle/PycharmProjects/Diabetic_Retinopathy_Detection/data/train'\n",
    "path_tfrecords_train = [os.path.join(data_dir, 'data_batch_%d.bin' % i) \n",
    "                        for i in xrange(0, 7)]\n",
    "# sampling parameters\n",
    "target_probs = np.array([77,  5, 12,  4,  2], dtype=np.float32)/100\n",
    "batch_size = 1000\n",
    "print(path_tfrecords_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def undersampling_filter(example):\n",
    "    \"\"\"\n",
    "    Computes if given example is rejected or not.\n",
    "    \"\"\"\n",
    "    class_prob = example['class_prob']\n",
    "    class_target_prob = example['class_target_prob']\n",
    "    prob_ratio = tf.cast(class_target_prob/class_prob, dtype=tf.float32)\n",
    "    prob_ratio = prob_ratio ** undersampling_coef\n",
    "    prob_ratio = tf.minimum(prob_ratio, 1.0)\n",
    "    \n",
    "    acceptance = tf.less_equal(tf.random_uniform([], dtype=tf.float32), prob_ratio)\n",
    "\n",
    "    return acceptance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_images(image_paths):\n",
    "    # Load the images from disk.\n",
    "    images = [imread(path) for path in image_paths]\n",
    "\n",
    "    # Convert to a numpy array and return it.\n",
    "    return np.asarray(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(serialized):\n",
    "    features = \\\n",
    "        {\n",
    "            'image_raw': tf.FixedLenFeature([], tf.string),\n",
    "            'label': tf.FixedLenFeature([], tf.int64)\n",
    "        }\n",
    "\n",
    "    # Parse the serialized data so we get a dict with our data.\n",
    "    parsed_example = tf.parse_single_example(serialized=serialized,\n",
    "                                             features=features)\n",
    "    # Get the image as raw bytes.\n",
    "    image_raw = parsed_example['image_raw']\n",
    "    # Decode the raw bytes so it becomes a tensor with type.\n",
    "    image = tf.decode_raw(image_raw, tf.uint8)\n",
    "    # The type is now uint8 but we need it to be float.\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = tf.divide(image, 255)\n",
    "    #\n",
    "    image = tf.reshape(image, [256, 256, 3])\n",
    "    # Get the label associated with the image.\n",
    "    label = parsed_example['label']\n",
    "    label = tf.cast(label, tf.int32)\n",
    "\n",
    "\n",
    "    # The image and label are now correct TensorFlow types.\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_fn(filenames, train, batch_size=batch_size, buffer_size=2048):\n",
    "    # Args:\n",
    "    # filenames:   Filenames for the TFRecords files.\n",
    "    # train:       Boolean whether training (True) or testing (False).\n",
    "    # batch_size:  Return batches of this size.\n",
    "    # buffer_size: Read buffers of this size. The random shuffling\n",
    "    #              is done on the buffer, so it must be big enough.\n",
    "\n",
    "    # Create a TensorFlow Dataset-object which has functionality\n",
    "    # for reading and shuffling data from TFRecords files.\n",
    "    dataset = tf.data.TFRecordDataset(filenames=filenames)\n",
    "\n",
    "    # Parse the serialized data in the TFRecords files.\n",
    "    # This returns TensorFlow tensors for the image and labels.\n",
    "    dataset = dataset.map(parse)\n",
    "\n",
    "    if train:\n",
    "        # If training then read a buffer of the given size and\n",
    "        # randomly shuffle it.\n",
    "        dataset = dataset.shuffle(buffer_size=buffer_size)\n",
    "\n",
    "        # Allow infinite reading of the data.\n",
    "        num_repeat = None\n",
    "    else:\n",
    "        # If testing then don't shuffle the data.\n",
    "        \n",
    "        # Only go through the data once.\n",
    "        num_repeat = 1\n",
    "    \n",
    "    #dataset = dataset.filter(undersampling_filter)\n",
    "\n",
    "    # Repeat the dataset the given number of times.\n",
    "    dataset = dataset.repeat(num_repeat)\n",
    "    \n",
    "    # Get a batch of data with the given size.\n",
    "    dataset = dataset.batch(batch_size)\n",
    "\n",
    "    # Create an iterator for the dataset and the above modifications.\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "\n",
    "    # Get the next batch of images and labels.\n",
    "    images_batch, labels_batch = iterator.get_next()\n",
    "\n",
    "#     # The input-function must return a dict wrapping the images.\n",
    "#     x = {'image': images_batch}\n",
    "#     y = labels_batch\n",
    "\n",
    "    return images_batch, labels_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_input_fn():\n",
    "    return input_fn(filenames=path_tfrecords_train, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = train_input_fn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range(0,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "classes = tf.constant(4)\n",
    "condition = tf.equal(y, classes)\n",
    "indices = tf.where(condition)\n",
    "number_class = tf.size(indices)\n",
    "class_prob = tf.divide(number_class,batch_size)\n",
    "\n",
    "mask = tf.one_hot(indices, depth=batch_size, dtype=tf.bool, on_value=True, off_value=False)\n",
    "\n",
    "\n",
    "x_sample = tf.gather(x,indices=tf.gather(indices,0),name=None)\n",
    "y_sample = tf.gather(y,indices=tf.gather(indices,1),name=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_elem(classes):\n",
    "    #get boolean true or false vector indicating where class is\n",
    "    condition = tf.equal(y, classes)\n",
    "    #get indecis for classes \n",
    "    indices = tf.where(condition)\n",
    "    #number of classes in original batch\n",
    "    number_class = tf.size(indices)\n",
    "    #class proportion\n",
    "    class_prob = tf.divide(number_class,batch_size)\n",
    "    #gather the first index that countaing the class\n",
    "    x_sample = tf.gather(x,indices=tf.gather(indices,0),name=None)\n",
    "    y_sample = tf.gather(y,indices=tf.gather(indices,0),name=None)\n",
    "    return(x_sample, y_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_one_from_each_class(x,y):\n",
    "    class_4 = tf.constant(4)\n",
    "    sample_4x, sample_4y  = return_elem(class_4)\n",
    "\n",
    "    class_3 = tf.constant(3)\n",
    "    class_2 = tf.constant(2)\n",
    "    sample_3x, sample_3y  = return_elem(class_3)\n",
    "    sample_2x, sample_2y  = return_elem(class_2)\n",
    "\n",
    "    class_1 = tf.constant(1)\n",
    "    class_0 = tf.constant(0)\n",
    "    sample_1x, sample_1y  = return_elem(class_1)\n",
    "    sample_0x, sample_0y  = return_elem(class_0)\n",
    "    \n",
    "    sample_y = tf.concat([sample_4y,sample_3y,sample_2y,sample_1y,sample_0y], axis=0)\n",
    "    sample_x = tf.concat([sample_4x,sample_3x,sample_2x,sample_1x,sample_0x], axis=0)\n",
    "\n",
    "    \n",
    "    return(sample_x,sample_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_5 = tf.constant(5)\n",
    "class_4 = tf.constant(4)\n",
    "sample_5x, sample_5y  = return_elem(class_5)\n",
    "sample_4x, sample_4y  = return_elem(class_4)\n",
    "\n",
    "class_3 = tf.constant(3)\n",
    "class_2 = tf.constant(2)\n",
    "sample_3x, sample_3y  = return_elem(class_3)\n",
    "sample_2x, sample_2y  = return_elem(class_2)\n",
    "\n",
    "class_1 = tf.constant(1)\n",
    "class_0 = tf.constant(0)\n",
    "sample_1x, sample_1y  = return_elem(class_1)\n",
    "sample_0x, sample_0y  = return_elem(class_0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([sample_5y,sample_4y,sample_3y,sample_2y,sample_1y,sample_0y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 24],\n",
      "       [ 39],\n",
      "       [121],\n",
      "       [176],\n",
      "       [270],\n",
      "       [424],\n",
      "       [444],\n",
      "       [474],\n",
      "       [547],\n",
      "       [551],\n",
      "       [573],\n",
      "       [619],\n",
      "       [827],\n",
      "       [929],\n",
      "       [967]]), array([4], dtype=int32)]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    #print(sess.run(x))\n",
    "    #f,lab = sess.run(extract_one_from_each_class(x,y))\n",
    "    print(sess.run([indices,y_sample]))\n",
    "    #d,e,f,lab = sess.run([sample_2y,sample_1y,sample_0y,y])\n",
    "    #print(a)\n",
    "    #print(b)\n",
    "    #print(c)\n",
    "    #print(d)\n",
    "    #print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    print(sess.run(x).shape)\n",
    "    #print(sess.run(x))\n",
    "    labels, mask_one, mask_two, ind, size,prop, x_sub, y_sub = sess.run([y, condition,mask, indices,number, class_prob, x_sample, y_sample])\n",
    "    print(labels)\n",
    "    print(mask)\n",
    "    print(ind)\n",
    "    print(size)\n",
    "    print(prop)\n",
    "    print(mask_two)\n",
    "    \n",
    "    print(x_sub)\n",
    "    print(y_sub)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = tf.placeholder(tf.string, shape=[None])\n",
    "dataset = tf.data.TFRecordDataset(path_tfrecords_train)\n",
    "dataset = dataset.map(parse)  # Parse the record into tensors.\n",
    "dataset = dataset.repeat()  # Repeat the input indefinitely.\n",
    "dataset = dataset.batch(32)\n",
    "iterator = dataset.make_initializable_iterator()\n",
    "\n",
    "# You can feed the initializer with the appropriate filenames for the current\n",
    "# phase of execution, e.g. training vs. validation.\n",
    "with tf.Session() as sess:\n",
    "    # Initialize `iterator` with training data.\n",
    "    training_filenames = path_tfrecords_train\n",
    "    sess.run(iterator.initializer, feed_dict={filenames: training_filenames})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inc_dataset = tf.data.Dataset.range(100)\n",
    "dec_dataset = tf.data.Dataset.range(0, -100, -1)\n",
    "dataset = tf.data.Dataset.zip((inc_dataset, dec_dataset))\n",
    "batched_dataset = dataset.batch(4)\n",
    "\n",
    "iterator = batched_dataset.make_one_shot_iterator()\n",
    "next_element = iterator.get_next()\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(next_element))  # ==> ([0, 1, 2,   3],   [ 0, -1,  -2,  -3])\n",
    "    print(sess.run(next_element))  # ==> ([4, 5, 6,   7],   [-4, -5,  -6,  -7])\n",
    "    print(sess.run(next_element))  # ==> ([8, 9, 10, 11],   [-8, -9, -10, -11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_element"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
